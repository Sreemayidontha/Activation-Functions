# Step 1: Import Required Libraries

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
# Use Subset to reduce dataset size
from torch.utils.data import Subset

import torch
from torch.utils.data import DataLoader, Subset
import numpy as np
import torchvision
import torchvision.transforms as transforms

# Define the transformation for the images
transform = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
)

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# Specify the number of samples you want to take (e.g., 10000)
num_samples = 100

# Randomly select indices for the subset
subset_indices = np.random.choice(len(trainset), num_samples, replace=False)

# Create a subset using the selected indices
train_subset = Subset(trainset, subset_indices)

# Create DataLoader for the subset
trainloader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2)

# If you want to take a subset from the test set as well, do the same for the test set
test_subset_indices = np.random.choice(len(testset), num_samples, replace=False)
test_subset = Subset(testset, test_subset_indices)
testloader = DataLoader(test_subset, batch_size=32, shuffle=False, num_workers=2)


# Step 3: Define Custom ResNet-20 Architecture
class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, activation=nn.ReLU()):
        super(BasicBlock, self).__init__()
        self.activation = activation
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = self.activation(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = self.activation(out)
        return out

class ResNet20(nn.Module):
    def __init__(self, num_classes=10, activation=nn.ReLU()):
        super(ResNet20, self).__init__()
        self.activation = activation
        self.in_channels = 16
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.layer1 = self._make_layer(16, 3)
        self.layer2 = self._make_layer(32, 3, stride=2)
        self.layer3 = self._make_layer(64, 3, stride=2)
        self.linear = nn.Linear(64, num_classes)

    def _make_layer(self, out_channels, blocks, stride=1):
        layers = [BasicBlock(self.in_channels, out_channels, stride, activation=self.activation)]
        self.in_channels = out_channels
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_channels, out_channels, activation=self.activation))
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.activation(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, 8)
        out = out.view(out.size(0), -1)
        print("Output shape before linear layer:", out.shape)  # Add this line
        out = self.linear(out)
        return out


# Step 4: Define Custom DenseNet-100 Architecture

class DenseBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, n_layers, activation=nn.ReLU()):
        super(DenseBlock, self).__init__()
        self.activation = activation
        self.layers = nn.ModuleList()
        self.activation = activation
        for i in range(n_layers):
            self.layers.append(self._make_layer(in_channels + i * growth_rate, growth_rate))

    def _make_layer(self, in_channels, growth_rate):
        layer = nn.Sequential(
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)
        )
        return layer

    def forward(self, x):
        for layer in self.layers:
            out = layer(x)
            x = torch.cat([x, out], 1)
        return x

class DenseNet100(nn.Module):
    def __init__(self, num_classes=10, growth_rate=12, activation=nn.ReLU()):
        super(DenseNet100, self).__init__()
        self.activation = activation
        self.growth_rate = growth_rate
        self.conv1 = nn.Conv2d(3, 2 * growth_rate, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(2 * growth_rate)
        self.dense1 = DenseBlock(2 * growth_rate, growth_rate, 6)
        self.trans1 = self._transition_layer(8 * growth_rate, 4 * growth_rate)
        self.dense2 = DenseBlock(4 * growth_rate, growth_rate, 12)
        self.trans2 = self._transition_layer(16 * growth_rate, 8 * growth_rate)
        self.dense3 = DenseBlock(8 * growth_rate, growth_rate, 24)
        self.trans3 = self._transition_layer(32 * growth_rate, 16 * growth_rate)
        self.dense4 = DenseBlock(16 * growth_rate, growth_rate, 16)

        # Calculate the output size after dense blocks and transitions
        # Adjust the input to the linear layer accordingly
        final_output_channels = 16 * growth_rate
        self.linear = nn.Linear(384, num_classes)

    def _transition_layer(self, in_channels, out_channels):
        layer = nn.Sequential(
            nn.BatchNorm2d(in_channels),
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.AvgPool2d(2)
        )
        return layer

    def forward(self, x):
        out = self.activation(self.bn1(self.conv1(x)))
        out = self.dense1(out)
        out = self.trans1(out)
        out = self.dense2(out)
        out = self.trans2(out)
        out = self.dense3(out)
        out = self.trans3(out)
        out = self.dense4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out

# Step 5: Define Mish and Swish Activation Functions

class Mish(nn.Module):
    def forward(self, x):
        return x * torch.tanh(F.softplus(x))

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

# Step 6: Initialize Models and Define Training/Evaluation Functions

def train(model, trainloader, criterion, optimizer, device):
    model.train()  # Set the model to training mode
    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()  # Zero the parameter gradients
        outputs = model(images)  # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backward pass (compute gradients)
        optimizer.step()  # Update the model weights

def evaluate(model, testloader, device):
    model.eval()  # Set the model to evaluation mode
    correct, total = 0, 0
    with torch.no_grad():  # Disable gradient calculation during evaluation
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total  # Calculate accuracy
    return accuracy


import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

class ResNet20(nn.Module):
    def __init__(self, num_classes=10,activation=None):
        super(ResNet20, self).__init__()
        # Set activation function (default is ReLU if not provided)
        self.activation = activation if activation is not None else nn.ReLU()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.block1 = self._make_block(16, 16, 3)
        self.block2 = self._make_block(16, 32, 3)
        self.block3 = self._make_block(32, 64, 3)
        self.fc = nn.Linear(64, num_classes)

    def _make_block(self, in_channels, out_channels, num_layers):
        layers = []
        for _ in range(num_layers):
            layers.append(self._residual_block(in_channels, out_channels))
            in_channels = out_channels
        return nn.Sequential(*layers)
    
    def _residual_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels)
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = x.mean([2, 3])  # Global Average Pooling
        x = self.fc(x)
        return x

# Initialize ResNet20 model
resnet_model = ResNet20(num_classes=10)  # Modify number of classes accordingly


import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

class DenseNet100(nn.Module):
    def __init__(self, num_classes=10,activation=None):
        super(DenseNet100, self).__init__()
        # Set activation function (default is ReLU if not provided)
        self.activation = activation if activation is not None else nn.ReLU()
        # Example: Use 100 layers of DenseNet architecture
        # Here we assume the architecture and layer sizes.
        # You can refer to the DenseNet paper for exact details or configurations.
        self.dense1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        self.dense2 = self._make_dense_block(64, 128, 6)  # Example: 6 layers in this dense block
        self.dense3 = self._make_dense_block(128, 256, 12)  # Example: 12 layers in this dense block
        self.fc = nn.Linear(256, num_classes)

    def _make_dense_block(self, in_channels, out_channels, num_layers):
        layers = []
        for _ in range(num_layers):
            layers.append(self._dense_layer(in_channels, out_channels))
            in_channels = out_channels
        return nn.Sequential(*layers)
    
    def _dense_layer(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.dense1(x)
        x = self.dense2(x)
        x = self.dense3(x)
        x = x.mean([2, 3])  # Global Average Pooling
        x = self.fc(x)
        return x

# Initialize DenseNet100 model
densenet_model = DenseNet100(num_classes=10)  # Modify number of classes accordingly


# Initialize the models (ResNet20 and DenseNet100)
resnet_model = ResNet20(num_classes=10)  # Modify the number of classes as per your dataset
densenet_model = DenseNet100(num_classes=10)  # Modify the number of classes

# Move the models to the device (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
resnet_model.to(device)
densenet_model.to(device)

# Define the loss function and optimizers
criterion = nn.CrossEntropyLoss()
optimizer_resnet = optim.SGD(resnet_model.parameters(), lr=0.01, momentum=0.9)
optimizer_densenet = optim.SGD(densenet_model.parameters(), lr=0.01, momentum=0.9)

# Step 6: Track Training and Validation Results
def train_and_evaluate(model, trainloader, testloader, criterion, optimizer, device, num_epochs=10):
    # Move model to device (GPU or CPU)
    model.to(device)
    
    train_losses = []  # List to track training losses
    test_accuracies = []  # List to track test accuracies
    
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode
        running_loss = 0.0
        for images, labels in trainloader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()  # Zero the gradients
            outputs = model(images)  # Forward pass
            loss = criterion(outputs, labels)  # Compute loss
            loss.backward()  # Backward pass
            optimizer.step()  # Update model weights
            running_loss += loss.item()  # Accumulate loss
        
        # Track training loss and test accuracy
        train_losses.append(running_loss / len(trainloader))  # Average training loss
        test_accuracy = evaluate(model, testloader, device)  # Evaluate on test set
        test_accuracies.append(test_accuracy)  # Store accuracy
        
        # Print training progress
        print(f"Epoch [{epoch+1}/{num_epochs}] - Loss: {train_losses[-1]:.4f}, Test Accuracy: {test_accuracy:.2f}%")
    
    return train_losses, test_accuracies

# After training the model, store the results for both models
train_losses_resnet, test_accuracies_resnet = train_and_evaluate(resnet_model, trainloader, testloader, criterion, optimizer_resnet, device)
train_losses_densenet, test_accuracies_densenet = train_and_evaluate(densenet_model, trainloader, testloader, criterion, optimizer_densenet, device)

# Step 7: Set up Device (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Step 8: Initialize the models with different activations (ReLU, Mish, and Swish)
# Initialize models with different activation functions
resnet_relu = ResNet20(activation=nn.ReLU()).to(device)
resnet_mish = ResNet20(activation=Mish()).to(device)
resnet_swish = ResNet20(activation=Swish()).to(device)

densenet_relu = DenseNet100(activation=nn.ReLU()).to(device)
densenet_mish = DenseNet100(activation=Mish()).to(device)
densenet_swish = DenseNet100(activation=Swish()).to(device)

# Step 9: Define Loss Function and Optimizer
criterion = nn.CrossEntropyLoss()

# Using Adam optimizer for all models
optimizer_resnet_relu = optim.Adam(resnet_relu.parameters(), lr=0.001)
optimizer_resnet_mish = optim.Adam(resnet_mish.parameters(), lr=0.001)
optimizer_resnet_swish = optim.Adam(resnet_swish.parameters(), lr=0.001)

optimizer_densenet_relu = optim.Adam(densenet_relu.parameters(), lr=0.001)
optimizer_densenet_mish = optim.Adam(densenet_mish.parameters(), lr=0.001)
optimizer_densenet_swish = optim.Adam(densenet_swish.parameters(), lr=0.001)

# Step 10: Train and Evaluate Models
num_epochs = 10

# Training loop
for epoch in range(num_epochs):
    # Train each model
    train(resnet_relu, trainloader, criterion, optimizer_resnet_relu, device)
    train(resnet_mish, trainloader, criterion, optimizer_resnet_mish, device)
    train(resnet_swish, trainloader, criterion, optimizer_resnet_swish, device)
    
    train(densenet_relu, trainloader, criterion, optimizer_densenet_relu, device)
    train(densenet_mish, trainloader, criterion, optimizer_densenet_mish, device)
    train(densenet_swish, trainloader, criterion, optimizer_densenet_swish, device)

    # Evaluate each model
    accuracy_resnet_relu = evaluate(resnet_relu, testloader, device)
    accuracy_resnet_mish = evaluate(resnet_mish, testloader, device)
    accuracy_resnet_swish = evaluate(resnet_swish, testloader, device)

    accuracy_densenet_relu = evaluate(densenet_relu, testloader, device)
    accuracy_densenet_mish = evaluate(densenet_mish, testloader, device)
    accuracy_densenet_swish = evaluate(densenet_swish, testloader, device)

    print(f"Epoch [{epoch+1}/{num_epochs}]")
    print(f"ResNet-20 (ReLU) Accuracy: {accuracy_resnet_relu:.2f}%")
    print(f"ResNet-20 (Mish) Accuracy: {accuracy_resnet_mish:.2f}%")
    print(f"ResNet-20 (Swish) Accuracy: {accuracy_resnet_swish:.2f}%")
    print(f"DenseNet-100 (ReLU) Accuracy: {accuracy_densenet_relu:.2f}%")
    print(f"DenseNet-100 (Mish) Accuracy: {accuracy_densenet_mish:.2f}%")
    print(f"DenseNet-100 (Swish) Accuracy: {accuracy_densenet_swish:.2f}%")

import matplotlib.pyplot as plt


# Step 7: Plot Experiment Results

def plot_results(train_losses_resnet, test_accuracies_resnet, train_losses_densenet, test_accuracies_densenet):
    # Plot Training Loss
    plt.figure(figsize=(6, 4))
    plt.plot(train_losses_resnet, label='ResNet20 Training Loss', color='blue')
    plt.plot(train_losses_densenet, label='DenseNet100 Training Loss', color='red')
    plt.title('Training Loss vs. Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot Test Accuracy
    plt.figure(figsize=(6, 4))
    plt.plot(test_accuracies_resnet, label='ResNet20 Test Accuracy', color='blue')
    plt.plot(test_accuracies_densenet, label='DenseNet100 Test Accuracy', color='red')
    plt.title('Test Accuracy vs. Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.show()

# Call the plotting function
plot_results(train_losses_resnet, test_accuracies_resnet, train_losses_densenet, test_accuracies_densenet)

